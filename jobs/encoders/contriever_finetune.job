#!/bin/bash

#SBATCH --partition=gpu_h100
#SBATCH --gpus=2
#SBATCH --job-name=contriever_finetune
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=10:00:00
#SBATCH --output=outputs/contriever_finetune.out
module purge
module load 2023
module load Anaconda3/2023.07-2

source activate hype

# Redirect ALL HuggingFace cache to scratch storage
export HF_HOME="/scratch-shared/scur1744/.cache/huggingface"
export HF_DATASETS_CACHE="/scratch-shared/scur1744/.cache/huggingface/datasets"
export TRANSFORMERS_CACHE="/scratch-shared/scur1744/.cache/huggingface/hub"
export PYTHONUNBUFFERED=1

#python hypencoder_cb/train/train.py hypencoder_cb/train/configs/tasb_freeze_encoder2.yaml

# Generate a random unused port for multi-GPU communication
export MASTER_PORT=$((RANDOM + 10000))
export MASTER_ADDR=127.0.0.1

torchrun --nproc_per_node=2 \
--master_port=${MASTER_PORT} \
hypencoder_cb/train/train.py hypencoder_cb/train/configs/contriever_freeze_encoder.yaml

torchrun --nproc_per_node=2 \
--master_port=${MASTER_PORT} \
hypencoder_cb/train/train.py hypencoder_cb/train/configs/contriever_nofreeze_encoder.yaml