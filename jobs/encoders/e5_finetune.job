#!/bin/bash

#SBATCH --partition=gpu_h100
#SBATCH --gpus=1
#SBATCH --job-name=e5_finetune
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=01:00:00
#SBATCH --output=outputs/e5_finetune.out

module purge
module load 2023
module load Anaconda3/2023.07-2

source activate hype

export HF_HOME="/scratch-shared/scur1744/.cache/huggingface"
export HF_DATASETS_CACHE="/scratch-shared/scur1744/.cache/huggingface/datasets"
export TRANSFORMERS_CACHE="/scratch-shared/scur1744/.cache/huggingface/hub"

python hypencoder_cb/train/train.py hypencoder_cb/tkrain/configs/e5_encoder.yaml
