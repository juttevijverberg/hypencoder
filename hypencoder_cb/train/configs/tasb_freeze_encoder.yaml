model_config:
  checkpoint_path: jfkback/hypencoder.6_layer
  tokenizer_pretrained_model_name_or_path: sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco
  query_encoder_kwargs:
    model_name_or_path: sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco
    freeze_transformer: true
    embedding_representation: null
    base_encoder_output_dim: 768
    converter_kwargs:
      vector_dimensions: [768, 768, 768, 768, 768, 768, 768, 1]
      activation_type: relu
      do_residual_on_last: false
  passage_encoder_kwargs:
    model_name_or_path: sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco
    freeze_transformer: true
    pooling_type: cls   # TAS-B still uses CLS output internally
  shared_encoder: true
  loss_type:
    - margin_mse
    - cross_entropy
  loss_kwargs:
    - {}
    - {"use_in_batch_negatives": true, "only_use_first_item": true}
data_config:
  training_data_jsonl: /scratch-shared/scur1744/tokenized_tasb.jsonl
  #training_huggingface_dataset: 'jfkback/hypencoder-msmarco-training-dataset'
  validation_data_jsonl: null
  positive_filter_type: first
  label_key: score
  num_positives_to_sample: 1
  num_negatives_to_sample: 7
trainer_config:
  hf_trainer_config:
    output_dir: /scratch-shared/scur1744/models/tasb_freeze
    overwrite_output_dir: true #Changed
    remove_unused_columns: false
    eval_strategy: 'no'
    per_device_train_batch_size: 64
    per_device_eval_batch_size: 64
    gradient_accumulation_steps: 1
    dataloader_prefetch_factor: 5
    dataloader_num_workers: 8
    dataloader_persistent_workers: false
    learning_rate: 2.0e-05 
    weight_decay: 0.01 #Changed
    num_train_epochs: 1
    lr_scheduler_type: linear
    warmup_ratio: 0.1 #changed
    warmup_steps: 0 #Changed
    logging_strategy: steps
    logging_steps: 10
    max_steps: -1
    save_strategy: steps
    save_steps: 500
    save_total_limit: null
    save_only_model: false
    bf16: true
    tf32: true
    fp16: false
    run_name: "hypencoder.tasb" #Changed
    disable_tqdm: true
    ddp_find_unused_parameters: true
    fsdp: ''
    fsdp_config: null
    report_to: none
    push_to_hub: false
    hub_model_id: null
    hub_strategy: every_save
    hub_private_repo: true
    gradient_checkpointing: false
    save_safetensors: false
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_epsilon: 1.0e-08
    torch_compile: false
  resume_from_checkpoint: false #Changed
