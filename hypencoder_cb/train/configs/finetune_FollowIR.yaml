# To do: change max document en query length.
model_config: # Reproducibility Study (RS): Model config from read me, with added checkpoint path
  checkpoint_path: jfkback/hypencoder.6_layer
  tokenizer_pretrained_model_name_or_path: google-bert/bert-base-uncased
  query_encoder_kwargs:
    model_name_or_path: google-bert/bert-base-uncased
    freeze_transformer: false
    embedding_representation: null
    base_encoder_output_dim: 768
    converter_kwargs:
      vector_dimensions: [768, 768, 768, 768, 768, 768, 768, 1]
      activation_type: relu
      do_residual_on_last: false
  passage_encoder_kwargs:
    model_name_or_path: google-bert/bert-base-uncased
    freeze_transformer: false
    pooling_type: cls
  shared_encoder: true
  loss_type: # RS: For fine-tuning, we only have cross-entropy loss.
    - cross_entropy
  loss_kwargs:
    - {"use_in_batch_negatives": true, "only_use_first_item": false}
data_config:
  training_data_jsonl: data/FollowIR/train/converted_train.jsonl
  validation_data_jsonl: null
  positive_filter_type: type
  positive_filter_kwargs:
    positive_type: positive
  num_positives_to_sample: 1
  num_negatives_to_sample: 1 # Change the number of negatives to sample based on how many you have
trainer_config: # RS: Training config of hard negatives
  hf_trainer_config:
    output_dir: outputs/hypencoder.contrastive_learning.hard_negative
    overwrite_output_dir: false
    remove_unused_columns: false
    evaluation_strategy: 'no'
    per_device_train_batch_size: 96 # RS: Changed batch-size from 64 to 96
    per_device_eval_batch_size: 64
    gradient_accumulation_steps: 1
    dataloader_prefetch_factor: 5
    dataloader_num_workers: 1
    dataloader_persistent_workers: false
    learning_rate: 8.0e-06 #RS: changed 2.0e-05 to 8.0e-06
    weight_decay: 0.0
    num_train_epochs: 1
    lr_scheduler_type: constant_with_warmup
    warmup_ratio: 1.0e-01 #RS: changed 0.0 to 1.0e-01
    warmup_steps: 6000
    logging_strategy: steps
    logging_steps: 10
    max_steps: 10000 # RS: Changed 1000000 to 3300
    save_strategy: steps
    save_steps: 2500
    save_total_limit: null
    save_only_model: false
    bf16: true
    tf32: true
    fp16: false
    run_name: "hypencoder.contrastive_learning.hard_negative"
    disable_tqdm: true
    ddp_find_unused_parameters: true
    fsdp: false
    fsdp_config: null
    report_to: none
    push_to_hub: false
    hub_model_id: null
    hub_strategy: every_save
    hub_private_repo: true
    gradient_checkpointing: false
    save_safetensors: false
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_epsilon: 1.0e-08
    torch_compile: false
  resume_from_checkpoint: false